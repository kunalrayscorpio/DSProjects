---
title: "AA_Opinion_Mining"
author: "Kunal_Ray"
date: "June 20, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#clear the environment

rm(list = ls(all=TRUE))

## Loading the rvest package

library(rvest)
```

```{r}
## Breaking down the URL required to be scraped split by page no.

rev1<-'https://www.g2crowd.com/products/adobe-analytics/reviews?page='
rev2<-'&variant=default'

## Creating an empty data frame to store URLs

url<-data.frame(PgNo=as.integer(),
                   URL=character(),
                stringsAsFactors = FALSE)

## Running a loop to create multiple URLs with varied page numbers

x<-1:3
for (i in seq_along(x)) {
  url[i,1]<-i
  url[i,2]<-paste(rev1,i,rev2,sep = "")
}

url ## View contents of the variable 'url'

rm(x,i,rev1,rev2) ## Removing unrequired variables
```

```{r}
## Reading all reviews & combining into a data frame

reviews<-data.frame(URL=character(),
                    REVIEWS=character(),
                    stringsAsFactors = FALSE) ## Creating empty data frame

x<-1:3
for (i in seq_along(x)) {
  reviews[i,1]<-url[i,2]
  cp1<-read_html(url[i,2]) ## Reading html code from website
  cp2<-html_nodes(cp1,'.formatted-text') ## Get all reviews
  cp3<-html_text(cp2) ## Convert to text
  reviews[i,2]<-paste(unlist(cp3), collapse =" ") ## Merge into single row per page
}

rm(url,cp1,cp2,cp3,i,x) ## Removing unrequired variables
```

```{r}
## Combining every review into a single character variable

fin_review<-paste(unlist(reviews[,2]),collapse = " ")

substr(fin_review, 1, 500) ## View the first 500 characters of review
```

```{r}
## Text Analytics ##

library(tidytext) ## Loading the tidytext library
library(dplyr) ## Loading the dplyr library

aareviews<-as.data.frame(fin_review,stringsAsFactors = FALSE) ## Creating a DF

tidyreviews<- aareviews %>%
  unnest_tokens(word,fin_review) ## Tokenizing & converting to tidy structure

head(tidyreviews) ## Viewing the first few rows in the tidy format
```

```{r}
## Removing stop words

data(stop_words) ## Load the stop words

custom_stop_words <- bind_rows(data_frame(word = c("adobe","analytics"), 
                                          lexicon = c("custom")), 
                               stop_words) ## Customizing list of stop words

tidyreviews <- tidyreviews %>%
  anti_join(custom_stop_words) ## Removing the stop words

tidyreviews %>%
  count(word,sort = TRUE) ## Sorting by most commonly occuring words

```

```{r}
## Plotting the top occuring words

library(ggplot2) ## Loading the ggplot library

tidyreviews%>%
  count(word,sort=TRUE)%>%
  filter(n > 50) %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(word,n))+
  geom_col()+
  xlab(NULL)+
  coord_flip()

```

```{r}
## Using bi-grams instead of words

aareviewsbigram<- aareviews %>%
  unnest_tokens(bigram,fin_review,token = "ngrams",n=2) ## Creating bigrams

aareviewsbigram %>%
  count(bigram,sort = TRUE) ## Checking the top bigrams

```

```{r}
## Need to remove those cases where either word in bigram is a stop word

library(tidyr) ## Loading tidyr library

bigrams_separated <- aareviewsbigram %>%
  separate(bigram,c("word1","word2"),sep = " ") ## separate bigrams into monograms

head(bigrams_separated) ## Checking data structure

bigrams_filtered<-bigrams_separated%>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) ## Filtering out cases where either is a stop word

bigram_counts<-bigrams_filtered%>%
  count(word1,word2,sort = TRUE) ## Getting frequency of occurence

bigram_counts%>%
  filter(n>3)## Getting the top list of bigrams

```

```{r}
## Using trigrams

aareviewstrigram<- aareviews %>%
  unnest_tokens(trigram,fin_review,token = "ngrams",n=3)%>%
  separate(trigram,c("word1","word2","word3"),sep = " ")%>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word)%>%
  count(word1,word2,word3,sort = TRUE) ## Creating trigram and removing stop words

aareviewstrigram %>%
  filter(n>1) ## Looking at the top trigram occurences

```

```{r}
## Analyzing bigrams

negation_words <- c("not", "no", "never", "without")

bigrams_separated%>%
  filter(word1 %in% negation_words) %>%
  count(word1,word2,sort = TRUE) ## Checking those bigrams where first word is negation

```

```{r}
## Plotting a network diagram to get a higher level picture

library(igraph) ## Loading the package igraph

bigram_graph <- bigram_counts %>%
  filter(n>5) %>% 
  graph_from_data_frame() ## Generate a bigram graph

bigram_graph ## Check the bigram graph

library(ggraph) ## Loading the package ggraph

set.seed(13)

ggraph(bigram_graph,layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name,vjust = 1, hjust = 1)) ## Graphing word connections

```
